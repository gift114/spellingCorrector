# evaluation/evaluation_runner.py
import os
import json
from typing import Dict, List
from .evaluator import YorubaSpellingEvaluator
from .test_generator import generate_test_files

def setup_evaluation_environment():
    """Set up the complete evaluation environment."""
    
    # Create necessary directories
    os.makedirs("evaluation/test_data", exist_ok=True)
    os.makedirs("evaluation/results", exist_ok=True)
    
    # Generate test data if it doesn't exist
    test_files_exist = any(
        os.path.exists(f"evaluation/test_data/{context}_tests.json") 
        for context in ['educational', 'conversational', 'literary']
    )
    
    if not test_files_exist:
        print("Generating test data...")
        generate_test_files()
    
    # Create a sample corpus file for enhanced corrector
    sample_corpus = """
    √†w·ªçn ·ªçm·ªç il√© k√†w√© l·ªç s√≠ il√© ·∫πÃÄk·ªçÃÅ. b√†b√° √†ti √¨y√° r√†w√© f√∫n √†w·ªçn ·ªçm·ªç w·ªçn.
    ow√≥ mi d√πn l√°ti r√≠ i·π£·∫πÃÅ tuntun. il√© n√°√† t√≥bi j√πl·ªç.
    mo f·∫πÃÅ k√†w√© n√≠ il√© ·∫πÃÄk·ªçÃÅ g√≠ga. ·ªçm·ªç n√°√† d√°ra p√∫p·ªçÃÄ.
    √¨w√© mi w√† n√≠ il√©. oko b√†b√° t√≥bi gan an.
    b√°wo ni o ·π£e w√† l√≥n√¨√≠? mo w√† n√≠ il√© ·∫πÃÄk·ªçÃÅ.
    i·π£·∫πÃÅ y√≠n d√πn o, ·∫π j·∫πÃÅ k√≠ n r√≠ i. al√°√†f√≠√† ni o, ·∫π k√∫ al√©.
    a d√∫p·∫πÃÅ o f√∫n √¨r√†nl·ªçÃÅw·ªçÃÅ y√≠n. √¨·π£·∫πÃÅ lo ≈Ñ ·π£e l√≥n√¨√≠?
    ow√≥ mi w√† n√≠ b√°nk√¨. il√© y√¨√≠ d√°ra gan an.
    √†k√≥k√≤ y√¨√≠ l√°gb√°ra f√∫n √¨d√†gb√†s√≥k√®. in√∫ √¨gb√† √†ti √¨simi.
    √¨t√†n √†r√≤s·ªç n√°√† d√πn l√°ti k√†. √†w·ªçn ak·∫πÃÅk·ªçÃÄ·ªçÃÅ n√°√† k√†w√© l√≥j√∫.
    oj√≠ ·ªçj·ªçÃÅ n√°√† f·∫πÃÅ w√© il√©. √¨gb√† owur·ªçÃÄ ni a ti l·ªç.
    or√≠ ire l'a ≈Ñ w√°. in√∫ d√≠d√πn ni √®mi √≥ fi h√†n.
    """
    
    with open("evaluation/sample_corpus.txt", "w", encoding="utf-8") as f:
        f.write(sample_corpus)
    
    print("‚úÖ Evaluation environment setup complete!")

def run_comprehensive_evaluation():
    """Run the complete evaluation with your lexicon."""
    
    # Setup environment
    setup_evaluation_environment()
    
    # Initialize evaluator with your lexicon
    lexicon_path = "data/yoruba_lexicon.txt"  # Your lexicon file
    corpus_path = "evaluation/sample_corpus.txt"
    
    evaluator = YorubaSpellingEvaluator(lexicon_path, corpus_path)
    
    # Run evaluation
    print("\n" + "="*60)
    print("RUNNING COMPREHENSIVE YOR√ôB√Å SPELLING CORRECTOR EVALUATION")
    print("="*60)
    
    results = evaluator.run_complete_evaluation()
    
    if results:
        # Print report
        print("\n" + results['report'])
        
        # Save detailed results
        output_file = "evaluation/results/evaluation_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            # Convert sets to lists for JSON serialization
            serializable_results = {
                'basic_results': results['basic_results'],
                'enhanced_results': results['enhanced_results'],
                'test_sets_size': {ctx: len(cases) for ctx, cases in results['test_sets'].items()}
            }
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"\n‚úÖ Detailed results saved to: {output_file}")
        
        # Generate performance summary
        generate_performance_summary(results)
    
    return results

def generate_performance_summary(results: Dict):
    """Generate a concise performance summary."""
    basic = results['basic_results']['overall']
    enhanced = results['enhanced_results']['overall']
    
    print("\n" + "="*50)
    print("PERFORMANCE SUMMARY")
    print("="*50)
    print(f"Overall Accuracy:    {basic['accuracy']:.3f} ‚Üí {enhanced['accuracy']:.3f}")
    print(f"F1 Score:           {basic['f1_score']:.3f} ‚Üí {enhanced['f1_score']:.3f}")
    print(f"Processing Time:    {basic['avg_processing_time']:.4f}s ‚Üí {enhanced['avg_processing_time']:.4f}s")
    
    improvement = enhanced['accuracy'] - basic['accuracy']
    print(f"\nImprovement: {improvement:+.3f} ({improvement*100:+.1f}%)")

if __name__ == "__main__":
    # Run the complete evaluation
    results = run_comprehensive_evaluation()
    
    # Additional analysis if results are available
    if results:
        print("\nüéØ Evaluation complete! Ready for Objective 5 (Application Development)")